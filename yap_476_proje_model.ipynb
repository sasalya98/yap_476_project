{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UwUwzSYaW47o",
        "outputId": "2bbaea7a-438d-40e5-e915-46aa8b1bdebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.7.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n"
          ]
        }
      ],
      "source": [
        "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU varsa kullan, yoksa CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRVIO8zKXYgP",
        "outputId": "a5077bb9-dac1-429c-b0f4-bf0471183ea8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df[df['TARGET_RI_24hr_30kt'] == 1.0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "z169cYPDaLUH",
        "outputId": "62b9ad48-6f64-4e00-ba15-741ff8f98347"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       STORM_ID STORM_NAME            TIMESTAMP RECORD_ID STATUS   LAT   LON  \\\n",
              "72     AL011856    UNNAMED  1856-08-09 00:00:00       NaN     HU  25.0 -83.9   \n",
              "73     AL011856    UNNAMED  1856-08-09 06:00:00       NaN     HU  25.7 -85.1   \n",
              "74     AL011856    UNNAMED  1856-08-09 12:00:00       NaN     HU  26.3 -86.5   \n",
              "75     AL011856    UNNAMED  1856-08-09 18:00:00       NaN     HU  27.0 -87.8   \n",
              "529    AL011886    UNNAMED  1886-06-13 06:00:00       NaN     TS  23.2 -95.7   \n",
              "...         ...        ...                  ...       ...    ...   ...   ...   \n",
              "55214  AL312020       IOTA  2020-11-15 00:00:00       NaN     TS  12.6 -76.7   \n",
              "55215  AL312020       IOTA  2020-11-15 06:00:00       NaN     HU  13.0 -77.1   \n",
              "55216  AL312020       IOTA  2020-11-15 12:00:00       NaN     HU  13.1 -78.0   \n",
              "55217  AL312020       IOTA  2020-11-15 18:00:00       NaN     HU  13.2 -78.9   \n",
              "55218  AL312020       IOTA  2020-11-16 00:00:00       NaN     HU  13.2 -79.8   \n",
              "\n",
              "       VMAX_current  MSLP_current  DAY_OF_YEAR  ...  STORM_SPEED_kts  \\\n",
              "72             70.0           NaN          222  ...              NaN   \n",
              "73             80.0           NaN          222  ...        12.916077   \n",
              "74             90.0           NaN          222  ...        13.949664   \n",
              "75            100.0           NaN          222  ...        13.573635   \n",
              "529            35.0           NaN          164  ...              NaN   \n",
              "...             ...           ...          ...  ...              ...   \n",
              "55214          55.0         992.0          320  ...         3.096446   \n",
              "55215          65.0         988.0          320  ...         5.590771   \n",
              "55216          70.0         982.0          320  ...         8.830352   \n",
              "55217          75.0         974.0          320  ...         8.826812   \n",
              "55218          90.0         961.0          321  ...         8.768117   \n",
              "\n",
              "       TARGET_DELV_24hr  TARGET_RI_24hr_30kt  DIST_TO_LAND_km    SST_avg  \\\n",
              "72                 40.0                  1.0       235.222505        NaN   \n",
              "73                 40.0                  1.0       305.204475        NaN   \n",
              "74                 40.0                  1.0       393.310591        NaN   \n",
              "75                 30.0                  1.0       260.742627        NaN   \n",
              "529                30.0                  1.0       210.335347        NaN   \n",
              "...                 ...                  ...              ...        ...   \n",
              "55214              35.0                  1.0       256.526635  28.951990   \n",
              "55215              55.0                  1.0       318.373849  28.906234   \n",
              "55216              65.0                  1.0       392.222541  28.928225   \n",
              "55217              55.0                  1.0       406.753123  29.013995   \n",
              "55218              40.0                  1.0       387.163172  28.962775   \n",
              "\n",
              "       MSLP_env_hPa  U10m_env_mps  V10m_env_mps  T2m_env_c  D2m_env_c  \n",
              "72              NaN           NaN           NaN        NaN        NaN  \n",
              "73              NaN           NaN           NaN        NaN        NaN  \n",
              "74              NaN           NaN           NaN        NaN        NaN  \n",
              "75              NaN           NaN           NaN        NaN        NaN  \n",
              "529             NaN           NaN           NaN        NaN        NaN  \n",
              "...             ...           ...           ...        ...        ...  \n",
              "55214           NaN           NaN           NaN        NaN        NaN  \n",
              "55215           NaN           NaN           NaN        NaN        NaN  \n",
              "55216           NaN           NaN           NaN        NaN        NaN  \n",
              "55217           NaN           NaN           NaN        NaN        NaN  \n",
              "55218           NaN           NaN           NaN        NaN        NaN  \n",
              "\n",
              "[1731 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b5caeb6-fe97-45c0-b183-9b9eab7995cf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STORM_ID</th>\n",
              "      <th>STORM_NAME</th>\n",
              "      <th>TIMESTAMP</th>\n",
              "      <th>RECORD_ID</th>\n",
              "      <th>STATUS</th>\n",
              "      <th>LAT</th>\n",
              "      <th>LON</th>\n",
              "      <th>VMAX_current</th>\n",
              "      <th>MSLP_current</th>\n",
              "      <th>DAY_OF_YEAR</th>\n",
              "      <th>...</th>\n",
              "      <th>STORM_SPEED_kts</th>\n",
              "      <th>TARGET_DELV_24hr</th>\n",
              "      <th>TARGET_RI_24hr_30kt</th>\n",
              "      <th>DIST_TO_LAND_km</th>\n",
              "      <th>SST_avg</th>\n",
              "      <th>MSLP_env_hPa</th>\n",
              "      <th>U10m_env_mps</th>\n",
              "      <th>V10m_env_mps</th>\n",
              "      <th>T2m_env_c</th>\n",
              "      <th>D2m_env_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>AL011856</td>\n",
              "      <td>UNNAMED</td>\n",
              "      <td>1856-08-09 00:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>25.0</td>\n",
              "      <td>-83.9</td>\n",
              "      <td>70.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>222</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>235.222505</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>AL011856</td>\n",
              "      <td>UNNAMED</td>\n",
              "      <td>1856-08-09 06:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>25.7</td>\n",
              "      <td>-85.1</td>\n",
              "      <td>80.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>222</td>\n",
              "      <td>...</td>\n",
              "      <td>12.916077</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>305.204475</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>AL011856</td>\n",
              "      <td>UNNAMED</td>\n",
              "      <td>1856-08-09 12:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>26.3</td>\n",
              "      <td>-86.5</td>\n",
              "      <td>90.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>222</td>\n",
              "      <td>...</td>\n",
              "      <td>13.949664</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>393.310591</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>AL011856</td>\n",
              "      <td>UNNAMED</td>\n",
              "      <td>1856-08-09 18:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>27.0</td>\n",
              "      <td>-87.8</td>\n",
              "      <td>100.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>222</td>\n",
              "      <td>...</td>\n",
              "      <td>13.573635</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>260.742627</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>AL011886</td>\n",
              "      <td>UNNAMED</td>\n",
              "      <td>1886-06-13 06:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TS</td>\n",
              "      <td>23.2</td>\n",
              "      <td>-95.7</td>\n",
              "      <td>35.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>164</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>210.335347</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55214</th>\n",
              "      <td>AL312020</td>\n",
              "      <td>IOTA</td>\n",
              "      <td>2020-11-15 00:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TS</td>\n",
              "      <td>12.6</td>\n",
              "      <td>-76.7</td>\n",
              "      <td>55.0</td>\n",
              "      <td>992.0</td>\n",
              "      <td>320</td>\n",
              "      <td>...</td>\n",
              "      <td>3.096446</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>256.526635</td>\n",
              "      <td>28.951990</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55215</th>\n",
              "      <td>AL312020</td>\n",
              "      <td>IOTA</td>\n",
              "      <td>2020-11-15 06:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>13.0</td>\n",
              "      <td>-77.1</td>\n",
              "      <td>65.0</td>\n",
              "      <td>988.0</td>\n",
              "      <td>320</td>\n",
              "      <td>...</td>\n",
              "      <td>5.590771</td>\n",
              "      <td>55.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>318.373849</td>\n",
              "      <td>28.906234</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55216</th>\n",
              "      <td>AL312020</td>\n",
              "      <td>IOTA</td>\n",
              "      <td>2020-11-15 12:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>13.1</td>\n",
              "      <td>-78.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>982.0</td>\n",
              "      <td>320</td>\n",
              "      <td>...</td>\n",
              "      <td>8.830352</td>\n",
              "      <td>65.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>392.222541</td>\n",
              "      <td>28.928225</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55217</th>\n",
              "      <td>AL312020</td>\n",
              "      <td>IOTA</td>\n",
              "      <td>2020-11-15 18:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>13.2</td>\n",
              "      <td>-78.9</td>\n",
              "      <td>75.0</td>\n",
              "      <td>974.0</td>\n",
              "      <td>320</td>\n",
              "      <td>...</td>\n",
              "      <td>8.826812</td>\n",
              "      <td>55.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>406.753123</td>\n",
              "      <td>29.013995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55218</th>\n",
              "      <td>AL312020</td>\n",
              "      <td>IOTA</td>\n",
              "      <td>2020-11-16 00:00:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HU</td>\n",
              "      <td>13.2</td>\n",
              "      <td>-79.8</td>\n",
              "      <td>90.0</td>\n",
              "      <td>961.0</td>\n",
              "      <td>321</td>\n",
              "      <td>...</td>\n",
              "      <td>8.768117</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>387.163172</td>\n",
              "      <td>28.962775</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1731 rows × 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b5caeb6-fe97-45c0-b183-9b9eab7995cf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9b5caeb6-fe97-45c0-b183-9b9eab7995cf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9b5caeb6-fe97-45c0-b183-9b9eab7995cf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b8f01b07-4850-4c89-b6b8-92fe7b24c7ff\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b8f01b07-4850-4c89-b6b8-92fe7b24c7ff')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b8f01b07-4850-4c89-b6b8-92fe7b24c7ff button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# 1) Veri yükleme\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "\n",
        "df = df.dropna()\n",
        "# 2) Hedef ve özellik ayrımı\n",
        "X = df.drop(columns=[\n",
        "    'STORM_ID','STORM_NAME','TIMESTAMP','RECORD_ID',\n",
        "    'TARGET_DELV_24hr','TARGET_RI_24hr_30kt'  # hedefleri çıkar\n",
        "])\n",
        "y = df['TARGET_RI_24hr_30kt']\n",
        "\n",
        "# 3) Kategorik ve sayısal sütun listeleri\n",
        "cat_cols = ['STATUS']                      # Tek kategorik sütun\n",
        "num_cols = [\n",
        "    'LAT','LON','VMAX_current','MSLP_current',\n",
        "    'DAY_OF_YEAR','STORM_AGE_hours',\n",
        "    'VMAX_prev_6hr','VMAX_prev_12hr','VMAX_prev_24hr',\n",
        "    'DELV_6hr','DELV_12hr','DELV_24hr',\n",
        "    'STORM_DIR','STORM_SPEED_kts',\n",
        "    'DIST_TO_LAND_km','SST_avg','MSLP_env_hPa',\n",
        "    'U10m_env_mps','V10m_env_mps','T2m_env_c','D2m_env_c'\n",
        "]\n",
        "\n",
        "# 4) Ön işleme pipeline’ı\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "])\n",
        "\n",
        "# 5) Eğitim/Test ayrımı (%80–%20 stratify ile)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 6) Model pipeline\n",
        "pipe = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', LogisticRegression(\n",
        "        class_weight='balanced',  # dengesiz sınıflar için\n",
        "        max_iter=1000\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 7) Basit hiperparametre araması\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10]\n",
        "}\n",
        "grid = GridSearchCV(\n",
        "    pipe, param_grid,\n",
        "    cv=5, scoring='f1', n_jobs=-1, verbose=1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 8) Değerlendirme\n",
        "y_pred = grid.predict(X_test)\n",
        "y_proba = grid.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"En iyi parametreler:\", grid.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52gizMUzXfop",
        "outputId": "9691ac3f-c63a-42aa-d39c-667bde3d9756"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En iyi parametreler: {'clf__C': 0.1}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.91      0.95        79\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.90        80\n",
            "   macro avg       0.49      0.46      0.47        80\n",
            "weighted avg       0.97      0.90      0.94        80\n",
            "\n",
            "ROC AUC: 0.45569620253164556\n",
            "Confusion Matrix:\n",
            " [[72  7]\n",
            " [ 1  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['TARGET_RI_24hr_30kt'] == 1.0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "3nsv3bhPYFZz",
        "outputId": "ee70f614-03c0-4cb1-fcac-5ef93633cb99"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       STORM_ID STORM_NAME            TIMESTAMP RECORD_ID STATUS   LAT   LON  \\\n",
              "13236  AL032014     BERTHA  2014-08-03 14:00:00         L     TS  21.8 -71.9   \n",
              "22403  AL052005      EMILY  2005-07-14 07:00:00         L     HU  12.0 -61.8   \n",
              "26830  AL062007      FELIX  2007-09-01 08:45:00         L     TS  12.1 -61.7   \n",
              "39417  AL102002    ISIDORE  2002-09-20 21:00:00         L     HU  22.0 -84.1   \n",
              "\n",
              "       VMAX_current  MSLP_current  DAY_OF_YEAR  ...  STORM_SPEED_kts  \\\n",
              "13236          40.0        1013.0          215  ...        14.639388   \n",
              "22403          75.0         989.0          195  ...        18.616557   \n",
              "26830          45.0        1001.0          244  ...        12.808704   \n",
              "39417          75.0         964.0          263  ...         4.412455   \n",
              "\n",
              "       TARGET_DELV_24hr  TARGET_RI_24hr_30kt  DIST_TO_LAND_km    SST_avg  \\\n",
              "13236              30.0                  1.0         3.333139  28.487430   \n",
              "22403              40.0                  1.0         2.151914  28.897088   \n",
              "26830              40.0                  1.0         0.000000  28.956930   \n",
              "39417              35.0                  1.0         0.000000  29.260316   \n",
              "\n",
              "       MSLP_env_hPa  U10m_env_mps  V10m_env_mps  T2m_env_c  D2m_env_c  \n",
              "13236   1015.263569     -5.608700      2.817946  27.270795  24.128691  \n",
              "22403   1010.643897     -4.710003      2.735216  26.960679  23.803574  \n",
              "26830   1012.669254     -4.609448      2.266512  25.839704  23.404455  \n",
              "39417   1003.035328     -4.936379     -0.527734  27.442039  24.686487  \n",
              "\n",
              "[4 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb518756-b232-4f0a-ba26-5f4ca14f626c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STORM_ID</th>\n",
              "      <th>STORM_NAME</th>\n",
              "      <th>TIMESTAMP</th>\n",
              "      <th>RECORD_ID</th>\n",
              "      <th>STATUS</th>\n",
              "      <th>LAT</th>\n",
              "      <th>LON</th>\n",
              "      <th>VMAX_current</th>\n",
              "      <th>MSLP_current</th>\n",
              "      <th>DAY_OF_YEAR</th>\n",
              "      <th>...</th>\n",
              "      <th>STORM_SPEED_kts</th>\n",
              "      <th>TARGET_DELV_24hr</th>\n",
              "      <th>TARGET_RI_24hr_30kt</th>\n",
              "      <th>DIST_TO_LAND_km</th>\n",
              "      <th>SST_avg</th>\n",
              "      <th>MSLP_env_hPa</th>\n",
              "      <th>U10m_env_mps</th>\n",
              "      <th>V10m_env_mps</th>\n",
              "      <th>T2m_env_c</th>\n",
              "      <th>D2m_env_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13236</th>\n",
              "      <td>AL032014</td>\n",
              "      <td>BERTHA</td>\n",
              "      <td>2014-08-03 14:00:00</td>\n",
              "      <td>L</td>\n",
              "      <td>TS</td>\n",
              "      <td>21.8</td>\n",
              "      <td>-71.9</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1013.0</td>\n",
              "      <td>215</td>\n",
              "      <td>...</td>\n",
              "      <td>14.639388</td>\n",
              "      <td>30.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.333139</td>\n",
              "      <td>28.487430</td>\n",
              "      <td>1015.263569</td>\n",
              "      <td>-5.608700</td>\n",
              "      <td>2.817946</td>\n",
              "      <td>27.270795</td>\n",
              "      <td>24.128691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22403</th>\n",
              "      <td>AL052005</td>\n",
              "      <td>EMILY</td>\n",
              "      <td>2005-07-14 07:00:00</td>\n",
              "      <td>L</td>\n",
              "      <td>HU</td>\n",
              "      <td>12.0</td>\n",
              "      <td>-61.8</td>\n",
              "      <td>75.0</td>\n",
              "      <td>989.0</td>\n",
              "      <td>195</td>\n",
              "      <td>...</td>\n",
              "      <td>18.616557</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.151914</td>\n",
              "      <td>28.897088</td>\n",
              "      <td>1010.643897</td>\n",
              "      <td>-4.710003</td>\n",
              "      <td>2.735216</td>\n",
              "      <td>26.960679</td>\n",
              "      <td>23.803574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26830</th>\n",
              "      <td>AL062007</td>\n",
              "      <td>FELIX</td>\n",
              "      <td>2007-09-01 08:45:00</td>\n",
              "      <td>L</td>\n",
              "      <td>TS</td>\n",
              "      <td>12.1</td>\n",
              "      <td>-61.7</td>\n",
              "      <td>45.0</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>244</td>\n",
              "      <td>...</td>\n",
              "      <td>12.808704</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.956930</td>\n",
              "      <td>1012.669254</td>\n",
              "      <td>-4.609448</td>\n",
              "      <td>2.266512</td>\n",
              "      <td>25.839704</td>\n",
              "      <td>23.404455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39417</th>\n",
              "      <td>AL102002</td>\n",
              "      <td>ISIDORE</td>\n",
              "      <td>2002-09-20 21:00:00</td>\n",
              "      <td>L</td>\n",
              "      <td>HU</td>\n",
              "      <td>22.0</td>\n",
              "      <td>-84.1</td>\n",
              "      <td>75.0</td>\n",
              "      <td>964.0</td>\n",
              "      <td>263</td>\n",
              "      <td>...</td>\n",
              "      <td>4.412455</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>29.260316</td>\n",
              "      <td>1003.035328</td>\n",
              "      <td>-4.936379</td>\n",
              "      <td>-0.527734</td>\n",
              "      <td>27.442039</td>\n",
              "      <td>24.686487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb518756-b232-4f0a-ba26-5f4ca14f626c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb518756-b232-4f0a-ba26-5f4ca14f626c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb518756-b232-4f0a-ba26-5f4ca14f626c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b2b6d754-02d9-4ce5-952d-1b5131472f6b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b2b6d754-02d9-4ce5-952d-1b5131472f6b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b2b6d754-02d9-4ce5-952d-1b5131472f6b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# 1) Veri yükleme\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "# 2) Özellik ve hedef\n",
        "X = df.drop(columns=[\n",
        "    'STORM_ID','STORM_NAME','TIMESTAMP','RECORD_ID',\n",
        "    'TARGET_DELV_24hr','TARGET_RI_24hr_30kt'\n",
        "])\n",
        "y = df['TARGET_RI_24hr_30kt']\n",
        "\n",
        "# 3) Sütun grupları\n",
        "cat_cols = ['STATUS']\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "# 4) Ön işleme\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "])\n",
        "\n",
        "# 5) Eğitim/test (stratify)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2,\n",
        "    random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 6) Model pipeline\n",
        "pipe_rf = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', RandomForestClassifier(\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 7) Hiperparametre araması\n",
        "param_grid = {\n",
        "    'clf__n_estimators': [100, 300, 500],\n",
        "    'clf__max_depth': [None, 10, 20],\n",
        "    'clf__min_samples_split': [2, 5]\n",
        "}\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_rf = GridSearchCV(\n",
        "    pipe_rf, param_grid,\n",
        "    cv=cv, scoring='f1', n_jobs=-1, verbose=1\n",
        ")\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "# 8) Değerlendirme\n",
        "y_pred = grid_rf.predict(X_test)\n",
        "y_proba = grid_rf.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"En iyi parametreler:\", grid_rf.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_VFvaUsYbdO",
        "outputId": "8ad5be9d-c63e-4ec9-9298-0d597aa6b36f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            "En iyi parametreler: {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      0.99        79\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.99        80\n",
            "   macro avg       0.49      0.50      0.50        80\n",
            "weighted avg       0.98      0.99      0.98        80\n",
            "\n",
            "ROC AUC: 0.3987341772151899\n",
            "Confusion Matrix:\n",
            " [[79  0]\n",
            " [ 1  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# 1) RandomOverSampler + RF pipeline\n",
        "pipe_rf_ros = ImbPipeline([\n",
        "    ('prep', preprocessor),                 # ColumnTransformer (aynı tanım)\n",
        "    ('ros', RandomOverSampler(random_state=42)),\n",
        "    ('clf', RandomForestClassifier(\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 2) Hiperparametre arama alanı (önceki grid’i kullanabilirsiniz)\n",
        "param_grid_ros = {\n",
        "    'clf__n_estimators': [100, 300, 500],\n",
        "    'clf__max_depth': [None, 10, 20],\n",
        "    'clf__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_rf_ros = GridSearchCV(\n",
        "    pipe_rf_ros, param_grid_ros,\n",
        "    cv=cv, scoring='f1', n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "# 3) Eğit & Değerlendir\n",
        "grid_rf_ros.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ros = grid_rf_ros.predict(X_test)\n",
        "y_proba_ros = grid_rf_ros.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"En iyi parametreler (ROS+RF):\", grid_rf_ros.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_ros))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_ros))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_ros))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa0KR5vKYtfs",
        "outputId": "75a7f7cf-cf9b-43d2-f122-57a3bf548c47"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            "En iyi parametreler (ROS+RF): {'clf__max_depth': None, 'clf__min_samples_split': 2, 'clf__n_estimators': 100}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      0.99        79\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.99        80\n",
            "   macro avg       0.49      0.50      0.50        80\n",
            "weighted avg       0.98      0.99      0.98        80\n",
            "\n",
            "ROC AUC: 0.4050632911392405\n",
            "Confusion Matrix:\n",
            " [[79  0]\n",
            " [ 1  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pozitif/negatif oranını hesaplayalım\n",
        "neg, pos = (y_train == 0).sum(), (y_train == 1).sum()\n",
        "scale = neg / pos\n",
        "print(f\"scale_pos_weight = {scale:.1f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBAyqF2eZpDE",
        "outputId": "56f5b376-2bfa-4743-fa37-e5a1b96b52e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scale_pos_weight = 105.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# 1) Pipeline: ön işleme aynı kaldı\n",
        "pipe_xgb = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='auc',\n",
        "        scale_pos_weight=scale,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 2) Hiperparametre ızgarası\n",
        "param_grid_xgb = {\n",
        "    'clf__n_estimators': [100, 200, 300],\n",
        "    'clf__max_depth':    [3, 6, 9],\n",
        "    'clf__learning_rate':[0.01, 0.1]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_xgb = GridSearchCV(\n",
        "    pipe_xgb, param_grid_xgb,\n",
        "    cv=cv, scoring='f1', n_jobs=-1, verbose=1\n",
        ")\n",
        "grid_xgb.fit(X_train, y_train)\n",
        "\n",
        "# 3) Değerlendirme\n",
        "y_proba_xgb = grid_xgb.predict_proba(X_test)[:,1]\n",
        "# Varsayılan eşik = 0.5\n",
        "y_pred_xgb = (y_proba_xgb >= 0.5).astype(int)\n",
        "\n",
        "print(\"En iyi parametreler (XGB):\", grid_xgb.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba_xgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNZ-AWuXZpcE",
        "outputId": "430a36d6-64d4-4823-d77c-4e6fab8cec58"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            "En iyi parametreler (XGB): {'clf__learning_rate': 0.01, 'clf__max_depth': 3, 'clf__n_estimators': 100}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98        79\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.96        80\n",
            "   macro avg       0.49      0.49      0.49        80\n",
            "weighted avg       0.97      0.96      0.97        80\n",
            "\n",
            "ROC AUC: 0.7278481012658228\n",
            "Confusion Matrix:\n",
            " [[77  2]\n",
            " [ 1  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:24:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix\n",
        "\n",
        "# 1) Olasılıklar\n",
        "y_proba_xgb = grid_xgb.predict_proba(X_test)[:,1]\n",
        "\n",
        "# 2) Precision‑Recall eğrisi ve F1 skoru hesaplama\n",
        "prec, rec, thresh = precision_recall_curve(y_test, y_proba_xgb)\n",
        "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-8)\n",
        "\n",
        "# 3) En iyi eşiği seçme\n",
        "best_ix = f1_scores.argmax()\n",
        "best_thr = thresh[best_ix]\n",
        "best_f1 = f1_scores[best_ix]\n",
        "print(f\"En iyi threshold: {best_thr:.3f} ile F1: {best_f1:.3f}\")\n",
        "\n",
        "# 4) Default eşikle (0.5) rapor\n",
        "y_pred_def = (y_proba_xgb >= 0.5).astype(int)\n",
        "print(\"\\n=== Default Threshold (0.5) ===\")\n",
        "print(classification_report(y_test, y_pred_def))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_def))\n",
        "\n",
        "# 5) Optimize eşikle rapor\n",
        "y_pred_opt = (y_proba_xgb >= best_thr).astype(int)\n",
        "print(f\"\\n=== Optimized Threshold ({best_thr:.3f}) ===\")\n",
        "print(classification_report(y_test, y_pred_opt))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_opt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYrOIix_Z1Hw",
        "outputId": "6b55f200-29ba-450a-f6c8-11025325fe13"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En iyi threshold: 0.191 ile F1: 0.071\n",
            "\n",
            "=== Default Threshold (0.5) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98        79\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.96        80\n",
            "   macro avg       0.49      0.49      0.49        80\n",
            "weighted avg       0.97      0.96      0.97        80\n",
            "\n",
            "Confusion Matrix:\n",
            " [[77  2]\n",
            " [ 1  0]]\n",
            "\n",
            "=== Optimized Threshold (0.191) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.67      0.80        79\n",
            "         1.0       0.04      1.00      0.07         1\n",
            "\n",
            "    accuracy                           0.68        80\n",
            "   macro avg       0.52      0.84      0.44        80\n",
            "weighted avg       0.99      0.68      0.79        80\n",
            "\n",
            "Confusion Matrix:\n",
            " [[53 26]\n",
            " [ 0  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 1) Aynı ön işleme adımları\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), num_cols),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
        "])\n",
        "\n",
        "# 2) Yalnızca negatif sınıfı kullanarak IsolationForest eğit\n",
        "X_neg = X_train[y_train == 0]\n",
        "iso_pipe = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('iso', IsolationForest(\n",
        "        n_estimators=100,\n",
        "        contamination=pos/(neg+pos),  # beklenen anomali oranı\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "iso_pipe.fit(X_neg)\n",
        "\n",
        "# 3) Test setinde anomali skorları al\n",
        "scores = iso_pipe.decision_function(X_test)      # pozitif → daha düşük skor\n",
        "y_pred_iso = iso_pipe.predict(X_test)\n",
        "# predict:  1 → inlier (negatif), -1 → outlier (pozitif)\n",
        "y_pred_iso = (y_pred_iso == -1).astype(int)      # outlier = 1\n",
        "\n",
        "# 4) Değerlendirme\n",
        "print(classification_report(y_test, y_pred_iso))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_iso))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfELdZsaaAHr",
        "outputId": "3838ddf8-4905-4cd1-8dfd-126ce0267a81"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.99      0.99        79\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.97        80\n",
            "   macro avg       0.49      0.49      0.49        80\n",
            "weighted avg       0.97      0.97      0.97        80\n",
            "\n",
            "Confusion Matrix:\n",
            " [[78  1]\n",
            " [ 1  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "missing = df.isna().mean().sort_values(ascending=False) * 100\n",
        "print(\"Sütun bazında eksik yüzdeleri:\\n\", missing)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eltwpwh_aXTC",
        "outputId": "055394aa-345e-4fa2-e686-5f701f10fa6a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sütun bazında eksik yüzdeleri:\n",
            " RECORD_ID              97.740359\n",
            "T2m_env_c              66.650371\n",
            "V10m_env_mps           66.650371\n",
            "U10m_env_mps           66.650371\n",
            "D2m_env_c              66.650371\n",
            "MSLP_env_hPa           66.650371\n",
            "SST_avg                64.685859\n",
            "MSLP_current           56.031142\n",
            "DELV_24hr              14.352707\n",
            "TARGET_DELV_24hr       14.352707\n",
            "TARGET_RI_24hr_30kt    14.352707\n",
            "VMAX_prev_24hr         14.253123\n",
            "DELV_12hr               7.260547\n",
            "VMAX_prev_12hr          7.160963\n",
            "DELV_6hr                3.711751\n",
            "VMAX_prev_6hr           3.615789\n",
            "STORM_SPEED_kts         3.604925\n",
            "STORM_DIR               3.604925\n",
            "VMAX_current            0.103205\n",
            "TIMESTAMP               0.000000\n",
            "STORM_AGE_hours         0.000000\n",
            "DAY_OF_YEAR             0.000000\n",
            "LAT                     0.000000\n",
            "LON                     0.000000\n",
            "STATUS                  0.000000\n",
            "STORM_ID                0.000000\n",
            "STORM_NAME              0.000000\n",
            "DIST_TO_LAND_km         0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# En üstte experimental import\n",
        "from sklearn.experimental import enable_iterative_imputer  # NOQA\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "\n",
        "# 1) Veri yükleme ve istemediğimiz sütunları atma\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "\n",
        "# Drop ID/timestamp ve RECORD_ID\n",
        "df = df.drop(columns=[\n",
        "    'RECORD_ID', 'STORM_ID', 'STORM_NAME', 'TIMESTAMP'\n",
        "])\n",
        "# Hedefi içermeyen satırları at\n",
        "df = df.dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "\n",
        "# 2) Özellik ve hedef\n",
        "X = df.drop(columns=['TARGET_DELV_24hr', 'TARGET_RI_24hr_30kt'])\n",
        "y = df['TARGET_RI_24hr_30kt'].astype(int)\n",
        "\n",
        "# 3) Eksik oranına göre kolon grupları\n",
        "high_missing = [\n",
        "    'T2m_env_c','V10m_env_mps','U10m_env_mps',\n",
        "    'D2m_env_c','MSLP_env_hPa','SST_avg','MSLP_current'\n",
        "]\n",
        "# Artık X.columns sadece sayısal ve STATUS içeriyor\n",
        "cat_cols = ['STATUS']\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "num_high = [c for c in high_missing if c in num_cols]\n",
        "num_low  = [c for c in num_cols if c not in num_high]\n",
        "\n",
        "# 4) Imputor tanımları\n",
        "high_imp = IterativeImputer(random_state=42)\n",
        "low_imp  = SimpleImputer(strategy='median')\n",
        "cat_imp  = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# 5) ColumnTransformer\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('high_num', Pipeline([\n",
        "        ('imp', high_imp),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), num_high),\n",
        "    ('low_num', Pipeline([\n",
        "        ('imp', low_imp),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), num_low),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', cat_imp),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "\n",
        "# 6) Eğitim/test ayrımı\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 7) scale_pos_weight hesap\n",
        "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
        "scale = neg / pos\n",
        "\n",
        "# 8) Pipeline & XGBoost + GridSearch\n",
        "pipe = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='auc',\n",
        "        scale_pos_weight=scale,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__n_estimators': [100, 200],\n",
        "    'clf__max_depth':    [3, 6],\n",
        "    'clf__learning_rate':[0.01, 0.1]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='f1',\n",
        "                    n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 9) Değerlendirme\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "y_proba = grid.predict_proba(X_test)[:,1]\n",
        "y_pred  = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLWSbZFwaYat",
        "outputId": "43aaa7ad-c0fc-45df-94f8-d02afc7c81c1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [16:31:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'clf__learning_rate': 0.1, 'clf__max_depth': 6, 'clf__n_estimators': 200}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94      9115\n",
            "           1       0.19      0.63      0.29       346\n",
            "\n",
            "    accuracy                           0.89      9461\n",
            "   macro avg       0.59      0.76      0.61      9461\n",
            "weighted avg       0.96      0.89      0.91      9461\n",
            "\n",
            "ROC AUC: 0.8825521039764854\n",
            "Confusion Matrix:\n",
            " [[8170  945]\n",
            " [ 127  219]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ace_tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUHbP45mc9C6",
        "outputId": "f7df9898-56ec-4e7d-ea02-96de9591b0c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ace_tools\n",
            "  Downloading ace_tools-0.0-py3-none-any.whl.metadata (300 bytes)\n",
            "Downloading ace_tools-0.0-py3-none-any.whl (1.1 kB)\n",
            "Installing collected packages: ace_tools\n",
            "Successfully installed ace_tools-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Data load & clean\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID', 'STORM_ID', 'STORM_NAME', 'TIMESTAMP'])\n",
        "df = df.dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "\n",
        "X = df.drop(columns=['TARGET_DELV_24hr', 'TARGET_RI_24hr_30kt'])\n",
        "y = df['TARGET_RI_24hr_30kt'].astype(int)\n",
        "\n",
        "# Splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Preprocessor\n",
        "cat_cols = ['STATUS']\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([('imp', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_cols),\n",
        "    ('cat', Pipeline([('imp', SimpleImputer(strategy='most_frequent')), ('enc', OneHotEncoder(handle_unknown='ignore'))]), cat_cols)\n",
        "])\n",
        "\n",
        "# Models\n",
        "models = {\n",
        "    'LogReg_L2': LogisticRegression(penalty='l2', solver='saga', class_weight='balanced', max_iter=1000, random_state=42),\n",
        "    'LogReg_L1': LogisticRegression(penalty='l1', solver='saga', class_weight='balanced', max_iter=1000, random_state=42),\n",
        "    'RidgeClf': RidgeClassifier(class_weight='balanced', random_state=42)\n",
        "}\n",
        "param_grids = {\n",
        "    'LogReg_L2': {'clf__C': [0.01, 0.1, 1, 10]},\n",
        "    'LogReg_L1': {'clf__C': [0.01, 0.1, 1, 10]},\n",
        "    'RidgeClf':  {'clf__alpha': [0.1, 1, 10]}\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    pipe = Pipeline([('prep', preprocessor), ('clf', model)])\n",
        "    grid = GridSearchCV(pipe, param_grids[name], cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    y_pred = grid.predict(X_test)\n",
        "    try:\n",
        "        y_score = grid.predict_proba(X_test)[:,1]\n",
        "        auc = roc_auc_score(y_test, y_score)\n",
        "    except:\n",
        "        auc = None\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'BestParams': grid.best_params_,\n",
        "        'F1_1': report['1']['f1-score'],\n",
        "        'Recall_1': report['1']['recall'],\n",
        "        'Precision_1': report['1']['precision'],\n",
        "        'ROC_AUC': auc\n",
        "    })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX6xRjW-atdA",
        "outputId": "d7818ef2-e9a6-4a54-83a3-bd5e4eaca61b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Model          BestParams      F1_1  Recall_1  Precision_1   ROC_AUC\n",
            "0  LogReg_L2    {'clf__C': 0.01}  0.151399  0.742775     0.084290  0.799109\n",
            "1  LogReg_L1     {'clf__C': 0.1}  0.151639  0.748555     0.084365  0.798669\n",
            "2   RidgeClf  {'clf__alpha': 10}  0.146409  0.765896     0.080941       NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Pipeline’ımız (önceki preprocessor’ı kullanıyoruz)\n",
        "pipe_dt = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', DecisionTreeClassifier(class_weight='balanced', random_state=42))\n",
        "])\n",
        "\n",
        "# Denenecek hiperparametreler\n",
        "param_grid_dt = {\n",
        "    'clf__max_depth': [None, 5, 10],\n",
        "    'clf__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# 5‑fold CV ile F1 optimizasyonu\n",
        "grid_dt = GridSearchCV(pipe_dt, param_grid_dt,\n",
        "                       cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_dt.fit(X_train, y_train)\n",
        "\n",
        "# Test seti değerlendirmesi\n",
        "y_pred = grid_dt.predict(X_test)\n",
        "y_proba = grid_dt.predict_proba(X_test)[:,1]\n",
        "print(\"En iyi params (DT):\", grid_dt.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKKQpWyXfPNu",
        "outputId": "1114e482-23ae-46f5-e99a-ab40f575f87e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
            "En iyi params (DT): {'clf__max_depth': None, 'clf__min_samples_split': 2}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97      9115\n",
            "           1       0.22      0.21      0.21       346\n",
            "\n",
            "    accuracy                           0.94      9461\n",
            "   macro avg       0.59      0.59      0.59      9461\n",
            "weighted avg       0.94      0.94      0.94      9461\n",
            "\n",
            "ROC AUC: 0.5885035148186785\n",
            "Confusion Matrix:\n",
            " [[8858  257]\n",
            " [ 275   71]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "# 4) RF pipeline + grid\n",
        "pipe_rf = Pipeline([('prep', preprocessor),\n",
        "                    ('clf', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))])\n",
        "\n",
        "param_grid_rf = {\n",
        "    'clf__n_estimators': [100, 300, 500],\n",
        "    'clf__max_depth': [None, 10, 20],\n",
        "    'clf__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=5, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "# 5) Evaluate\n",
        "y_pred = grid_rf.predict(X_test)\n",
        "y_proba = grid_rf.predict_proba(X_test)[:,1]\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "print(df_report)\n",
        "print(\"ROC AUC:\", auc)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Best params (RF):\", grid_rf.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgrvH4Xuf4Y4",
        "outputId": "9d9a020c-f604-4aa5-ee50-5fb93d5a5bed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "              precision    recall  f1-score      support\n",
            "0              0.973748  0.964454  0.969079  9115.000000\n",
            "1              0.251732  0.315029  0.279846   346.000000\n",
            "accuracy       0.940704  0.940704  0.940704     0.940704\n",
            "macro avg      0.612740  0.639742  0.624462  9461.000000\n",
            "weighted avg   0.947343  0.940704  0.943873  9461.000000\n",
            "ROC AUC: 0.8737747281841847\n",
            "Confusion Matrix:\n",
            " [[8791  324]\n",
            " [ 237  109]]\n",
            "Best params (RF): {'clf__max_depth': 20, 'clf__min_samples_split': 5, 'clf__n_estimators': 500}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "pipe_xgb_gpu = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', XGBClassifier(\n",
        "        tree_method='gpu_hist',\n",
        "        predictor='gpu_predictor',\n",
        "        scale_pos_weight=scale,\n",
        "        random_state=42,\n",
        "        n_jobs=1\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'clf__n_estimators': [100, 200, 300],\n",
        "    'clf__max_depth':    [3, 6, 9],\n",
        "    'clf__learning_rate':[0.01, 0.1]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_xgb_gpu = GridSearchCV(\n",
        "    pipe_xgb_gpu, param_grid_xgb,\n",
        "    cv=cv, scoring='f1', n_jobs=1, verbose=2\n",
        ")\n",
        "grid_xgb_gpu.fit(X_train, y_train)\n",
        "\n",
        "# Değerlendirme\n",
        "y_pred = grid_xgb_gpu.predict(X_test)\n",
        "y_proba = grid_xgb_gpu.predict_proba(X_test)[:,1]\n",
        "print(\"Best XGB GPU params:\", grid_xgb_gpu.best_params_)\n",
        "print(\"XGB GPU ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-x1w23fmkx1Q",
        "outputId": "8bd83ab0-4043-4864-ea5a-60e879a72387"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:729: UserWarning: [17:14:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  return func(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=100; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=200; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=200; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=200; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=200; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=200; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=300; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=300; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=300; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:14:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:14:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=300; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:00] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=3, clf__n_estimators=300; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:00] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=200; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=200; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=200; total time=   0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=200; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=200; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=300; total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=300; total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=300; total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=300; total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=6, clf__n_estimators=300; total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=100; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=100; total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=100; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=100; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=100; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=200; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=200; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=200; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=200; total time=   1.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=200; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=300; total time=   1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=300; total time=   1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=300; total time=   1.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=300; total time=   1.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__max_depth=9, clf__n_estimators=300; total time=   1.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=100; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=200; total time=   0.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=200; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=200; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=200; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=200; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=300; total time=   0.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=300; total time=   0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=300; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=300; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=3, clf__n_estimators=300; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=100; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=100; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=100; total time=   0.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=200; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=200; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=200; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=200; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=200; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=300; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=300; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=300; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=300; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=6, clf__n_estimators=300; total time=   0.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=100; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=100; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=100; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=100; total time=   0.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=100; total time=   0.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=200; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=200; total time=   1.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=200; total time=   0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=200; total time=   0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=200; total time=   0.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=300; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=300; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=300; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=300; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__max_depth=9, clf__n_estimators=300; total time=   1.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [17:15:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best XGB GPU params: {'clf__learning_rate': 0.1, 'clf__max_depth': 9, 'clf__n_estimators': 300}\n",
            "XGB GPU ROC AUC: 0.9043046620098358\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      9115\n",
            "           1       0.49      0.36      0.42       346\n",
            "\n",
            "    accuracy                           0.96      9461\n",
            "   macro avg       0.73      0.67      0.70      9461\n",
            "weighted avg       0.96      0.96      0.96      9461\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [17:15:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "pipe_lgb_gpu = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', LGBMClassifier(\n",
        "        device='gpu',\n",
        "        gpu_device_id=0,\n",
        "        scale_pos_weight=scale,\n",
        "        random_state=42,\n",
        "        n_jobs=1\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_lgb = {\n",
        "    'clf__n_estimators': [100, 300, 500],\n",
        "    'clf__num_leaves':    [31, 63, 127],\n",
        "    'clf__learning_rate':[0.01, 0.1]\n",
        "}\n",
        "\n",
        "grid_lgb_gpu = GridSearchCV(\n",
        "    pipe_lgb_gpu, param_grid_lgb,\n",
        "    cv=cv, scoring='f1', n_jobs=1, verbose=2\n",
        ")\n",
        "grid_lgb_gpu.fit(X_train, y_train)\n",
        "\n",
        "# Değerlendirme\n",
        "y_pred = grid_lgb_gpu.predict(X_test)\n",
        "y_proba = grid_lgb_gpu.predict_proba(X_test)[:,1]\n",
        "print(\"Best LightGBM GPU params:\", grid_lgb_gpu.best_params_)\n",
        "print(\"LightGBM GPU ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D-lljfdlhOL",
        "outputId": "d1d0186c-f4d2-4783-897f-c6df51d47112"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001725 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=31; total time=   7.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001681 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=31; total time=   1.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001582 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=31; total time=   1.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001548 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=31; total time=   1.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001573 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=31; total time=   1.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001612 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=63; total time=   1.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001571 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=63; total time=   1.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.005311 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=63; total time=   1.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.003698 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=63; total time=   1.6s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001635 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=63; total time=   1.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001544 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=127; total time=   2.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001576 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=127; total time=   2.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.009785 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=127; total time=   3.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002836 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=127; total time=   2.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001562 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=100, clf__num_leaves=127; total time=   2.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002816 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=31; total time=   2.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001558 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=31; total time=   2.4s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001617 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=31; total time=   2.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.003657 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=31; total time=   2.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001597 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=31; total time=   2.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001571 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=63; total time=   4.4s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001584 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=63; total time=   4.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001694 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=63; total time=   3.6s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001574 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=63; total time=   3.6s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001599 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=63; total time=   4.2s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001596 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=127; total time=   6.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001636 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=127; total time=   6.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001639 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=127; total time=   5.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001594 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=127; total time=   6.6s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001578 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=300, clf__num_leaves=127; total time=   5.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001649 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=31; total time=   3.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002922 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=31; total time=   4.2s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001568 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=31; total time=   3.6s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001548 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=31; total time=   3.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001609 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=31; total time=   4.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001567 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=63; total time=   5.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001733 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=63; total time=   6.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001614 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=63; total time=   5.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001586 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=63; total time=   6.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001637 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=63; total time=   5.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001561 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=127; total time=  11.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001591 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=127; total time=  10.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001645 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=127; total time=  10.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001613 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=127; total time=  10.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001567 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.01, clf__n_estimators=500, clf__num_leaves=127; total time=  10.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001588 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=31; total time=   1.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001630 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001596 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001933 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001570 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=31; total time=   1.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001624 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=63; total time=   1.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001575 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=63; total time=   1.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001559 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=63; total time=   1.4s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001597 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=63; total time=   1.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002885 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=63; total time=   1.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001632 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=127; total time=   2.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001628 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=127; total time=   2.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001583 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=127; total time=   2.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001604 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=127; total time=   2.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.003190 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=100, clf__num_leaves=127; total time=   2.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001828 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=31; total time=   2.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001567 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=31; total time=   2.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001662 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=31; total time=   2.2s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001586 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=31; total time=   2.2s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001847 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=31; total time=   2.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002913 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=63; total time=   3.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001595 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=63; total time=   3.4s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001629 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=63; total time=   3.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001648 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=63; total time=   4.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001593 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=63; total time=   3.4s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001603 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=127; total time=   5.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.003017 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=127; total time=   6.4s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001518 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=127; total time=   5.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002963 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=127; total time=   6.4s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001714 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=300, clf__num_leaves=127; total time=   5.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002997 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=31; total time=   3.9s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001638 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=31; total time=   3.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001683 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=31; total time=   3.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001626 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=31; total time=   3.7s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.004014 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=31; total time=   3.5s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001613 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=63; total time=   5.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001606 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=63; total time=   6.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001588 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=63; total time=   5.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001641 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=63; total time=   6.0s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001638 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=63; total time=   5.3s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001620 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=127; total time=  10.2s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3551\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001683 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=127; total time=  10.2s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001579 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=127; total time=  10.1s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3554\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.003892 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=127; total time=   9.8s\n",
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3525\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001592 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END clf__learning_rate=0.1, clf__n_estimators=500, clf__num_leaves=127; total time=  10.2s\n",
            "[LightGBM] [Info] Number of positive: 1385, number of negative: 36457\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3555\n",
            "[LightGBM] [Info] Number of data points in the train set: 37842, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.87 MB) transferred to GPU in 0.001984 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270433\n",
            "[LightGBM] [Info] Start training from score -3.270433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best LightGBM GPU params: {'clf__learning_rate': 0.1, 'clf__n_estimators': 500, 'clf__num_leaves': 31}\n",
            "LightGBM GPU ROC AUC: 0.9049575272925591\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98      9115\n",
            "           1       0.38      0.47      0.42       346\n",
            "\n",
            "    accuracy                           0.95      9461\n",
            "   macro avg       0.68      0.72      0.70      9461\n",
            "weighted avg       0.96      0.95      0.96      9461\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qIOQq06mlpSY",
        "outputId": "a9778ba5-2e18-4604-9792-e3e14a7ff158"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "pipe_cb_gpu = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', CatBoostClassifier(\n",
        "        task_type='GPU',\n",
        "        devices='0',\n",
        "        scale_pos_weight=scale,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_cb = {\n",
        "    'clf__iterations':    [100, 300, 500],\n",
        "    'clf__depth':         [4, 6, 8],\n",
        "    'clf__learning_rate': [0.01, 0.1]\n",
        "}\n",
        "\n",
        "grid_cb_gpu = GridSearchCV(\n",
        "    pipe_cb_gpu, param_grid_cb,\n",
        "    cv=cv, scoring='f1', n_jobs=1, verbose=2\n",
        ")\n",
        "grid_cb_gpu.fit(X_train, y_train)\n",
        "\n",
        "# Değerlendirme\n",
        "y_pred = grid_cb_gpu.predict(X_test)\n",
        "y_proba = grid_cb_gpu.predict_proba(X_test)[:,1]\n",
        "print(\"Best CatBoost GPU params:\", grid_cb_gpu.best_params_)\n",
        "print(\"CatBoost GPU ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShWAL3P-lndy",
        "outputId": "b498ce56-4738-4560-8268-5229336353b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.01; total time=   5.5s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.01; total time=   4.2s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.01; total time=   3.4s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.01; total time=   4.6s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.01; total time=   2.0s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.1; total time=   1.4s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.1; total time=   1.4s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.1; total time=   1.4s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.1; total time=   1.4s\n",
            "[CV] END clf__depth=4, clf__iterations=100, clf__learning_rate=0.1; total time=   1.4s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.01; total time=   4.3s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.01; total time=   2.3s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.01; total time=   2.2s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.01; total time=   2.2s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.01; total time=   2.2s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.1; total time=   3.9s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.1; total time=   2.2s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.1; total time=   2.2s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.1; total time=   2.2s\n",
            "[CV] END clf__depth=4, clf__iterations=300, clf__learning_rate=0.1; total time=   2.2s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.01; total time=  12.3s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.01; total time=  15.8s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.01; total time=  12.6s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.01; total time=  12.6s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.01; total time=  13.0s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.1; total time=  12.4s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.1; total time=  12.4s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.1; total time=  12.3s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.1; total time=  12.1s\n",
            "[CV] END clf__depth=4, clf__iterations=500, clf__learning_rate=0.1; total time=  12.4s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.01; total time=   1.5s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.01; total time=   1.6s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.01; total time=   1.5s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.01; total time=   1.5s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.01; total time=   1.5s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.1; total time=   3.2s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.1; total time=   1.5s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.1; total time=   1.6s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.1; total time=   1.5s\n",
            "[CV] END clf__depth=6, clf__iterations=100, clf__learning_rate=0.1; total time=   1.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.01; total time=   2.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.01; total time=   4.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.01; total time=   2.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.01; total time=   3.1s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.01; total time=   2.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.1; total time=   4.4s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.1; total time=   2.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.1; total time=   2.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.1; total time=   2.5s\n",
            "[CV] END clf__depth=6, clf__iterations=300, clf__learning_rate=0.1; total time=   4.5s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.01; total time=  18.5s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.01; total time=  19.5s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.01; total time=  18.1s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.01; total time=  19.5s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.01; total time=  18.0s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.1; total time=  19.5s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.1; total time=  17.3s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.1; total time=  19.8s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.1; total time=  18.3s\n",
            "[CV] END clf__depth=6, clf__iterations=500, clf__learning_rate=0.1; total time=  20.0s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.01; total time=   1.8s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.01; total time=   1.8s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.01; total time=   2.0s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.01; total time=   3.0s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.01; total time=   1.8s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.1; total time=   1.8s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.1; total time=   1.8s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.1; total time=   1.8s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.1; total time=   1.9s\n",
            "[CV] END clf__depth=8, clf__iterations=100, clf__learning_rate=0.1; total time=   3.3s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.01; total time=   3.4s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.01; total time=   3.4s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.01; total time=   5.3s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.01; total time=   3.4s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.01; total time=   3.4s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.1; total time=   4.6s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.1; total time=   3.6s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.1; total time=   3.5s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.1; total time=   3.4s\n",
            "[CV] END clf__depth=8, clf__iterations=300, clf__learning_rate=0.1; total time=   5.1s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.01; total time=  43.0s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.01; total time=  36.4s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.01; total time=  46.2s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.01; total time=  44.7s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.01; total time=  42.8s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.1; total time=  50.1s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.1; total time=  45.6s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.1; total time=  48.8s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.1; total time=  49.4s\n",
            "[CV] END clf__depth=8, clf__iterations=500, clf__learning_rate=0.1; total time=  49.8s\n",
            "Best CatBoost GPU params: {'clf__depth': 8, 'clf__iterations': 300, 'clf__learning_rate': 0.1}\n",
            "CatBoost GPU ROC AUC: 0.8928362383037552\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      9115\n",
            "           1       0.29      0.51      0.37       346\n",
            "\n",
            "    accuracy                           0.94      9461\n",
            "   macro avg       0.64      0.73      0.67      9461\n",
            "weighted avg       0.96      0.94      0.94      9461\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# SVM pipeline\n",
        "pipe_svc = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('clf', SVC(class_weight='balanced', probability=True, random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid_svc = {\n",
        "    'clf__kernel': ['linear', 'rbf'],\n",
        "    'clf__C': [0.1, 1, 10],\n",
        "    'clf__gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_svc = GridSearchCV(pipe_svc, param_grid_svc, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
        "grid_svc.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation\n",
        "y_pred = grid_svc.predict(X_test)\n",
        "y_proba = grid_svc.predict_proba(X_test)[:,1]\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Display results\n",
        "print(df_report)\n",
        "print(\"Best SVM Params:\", grid_svc.best_params_)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Z4K2wnfgrfAB",
        "outputId": "35f6c4fe-f8ad-4313-9b69-964d5ecf94b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-100575067.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mgrid_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_svc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_svc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mgrid_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "# 1) Load & clean raw data\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_ID','STORM_NAME','TIMESTAMP'])\n",
        "df = df.dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "\n",
        "# 2) Separate features and target\n",
        "X_raw = df.drop(columns=['TARGET_DELV_24hr','TARGET_RI_24hr_30kt'])\n",
        "y_raw = df['TARGET_RI_24hr_30kt'].astype(int)\n",
        "\n",
        "# 3) Build preprocessing pipeline\n",
        "cat_cols = ['STATUS']\n",
        "num_cols = [c for c in X_raw.columns if c not in cat_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), num_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "\n",
        "# 4) Fit and transform features\n",
        "X_proc = preprocessor.fit_transform(X_raw)  # numpy array of floats\n",
        "\n",
        "# 5) Split into train and test\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X_proc, y_raw.values, test_size=0.2, stratify=y_raw, random_state=42\n",
        ")\n",
        "\n",
        "# 6) Further split train into train/validation\n",
        "train_ds_full = TensorDataset(\n",
        "    torch.from_numpy(X_train_full).float(),\n",
        "    torch.from_numpy(y_train_full).long()\n",
        ")\n",
        "n_train = int(len(train_ds_full) * 0.8)\n",
        "n_val = len(train_ds_full) - n_train\n",
        "train_ds, val_ds = random_split(\n",
        "    train_ds_full, [n_train, n_val],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# 7) Prepare device and class weights\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "neg = int((y_train_full == 0).sum())\n",
        "pos = int((y_train_full == 1).sum())\n",
        "class_weight = torch.tensor([1.0, neg/pos], device=device)\n",
        "\n",
        "# 8) Define hyperparameter grid\n",
        "grid = {\n",
        "    'hidden_sizes': [(64,), (128,), (64, 64)],\n",
        "    'dropout': [0.2, 0.4],\n",
        "    'lr': [1e-3, 1e-2]\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "# 9) Grid search loop\n",
        "for hs in grid['hidden_sizes']:\n",
        "    for dr in grid['dropout']:\n",
        "        for lr in grid['lr']:\n",
        "            # Build MLP model\n",
        "            layers = []\n",
        "            inp_dim = X_proc.shape[1]\n",
        "            for h in hs:\n",
        "                layers += [nn.Linear(inp_dim, h), nn.ReLU(), nn.Dropout(dr)]\n",
        "                inp_dim = h\n",
        "            layers.append(nn.Linear(inp_dim, 2))\n",
        "            model = nn.Sequential(*layers).to(device)\n",
        "\n",
        "            # Set up loss and optimizer\n",
        "            criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # DataLoaders\n",
        "            train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "            val_loader = DataLoader(val_ds, batch_size=64)\n",
        "\n",
        "            # Train for 5 epochs\n",
        "            for epoch in range(5):\n",
        "                model.train()\n",
        "                for xb, yb in train_loader:\n",
        "                    xb, yb = xb.to(device), yb.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = criterion(model(xb), yb)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            # Validation evaluation\n",
        "            model.eval()\n",
        "            all_preds, all_probs, all_labels = [], [], []\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb = xb.to(device)\n",
        "                    logits = model(xb)\n",
        "                    probs = torch.softmax(logits, dim=1)[:, 1]\n",
        "                    preds = (probs > 0.5).long().cpu()\n",
        "                    all_preds.extend(preds.tolist())\n",
        "                    all_probs.extend(probs.cpu().tolist())\n",
        "                    all_labels.extend(yb.tolist())\n",
        "\n",
        "            # Metrics\n",
        "            val_f1 = f1_score(all_labels, all_preds)\n",
        "            val_auc = roc_auc_score(all_labels, all_probs)\n",
        "            results.append({\n",
        "                'hidden_sizes': hs,\n",
        "                'dropout': dr,\n",
        "                'lr': lr,\n",
        "                'val_f1': val_f1,\n",
        "                'val_auc': val_auc\n",
        "            })\n",
        "\n",
        "# 10) Display results\n",
        "import pandas as pd\n",
        "df_results = pd.DataFrame(results).sort_values('val_f1', ascending=False)\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez9P1F0zryzs",
        "outputId": "3b55f319-547c-4227-ba8e-30008373a850"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   hidden_sizes  dropout     lr    val_f1   val_auc\n",
            "7        (128,)      0.4  0.010  0.232700  0.828035\n",
            "10     (64, 64)      0.4  0.001  0.221507  0.845155\n",
            "1         (64,)      0.2  0.010  0.210654  0.836509\n",
            "0         (64,)      0.2  0.001  0.205694  0.840619\n",
            "2         (64,)      0.4  0.001  0.205156  0.839467\n",
            "6        (128,)      0.4  0.001  0.197942  0.845911\n",
            "4        (128,)      0.2  0.001  0.195243  0.845618\n",
            "9      (64, 64)      0.2  0.010  0.186095  0.826895\n",
            "8      (64, 64)      0.2  0.001  0.178883  0.840505\n",
            "11     (64, 64)      0.4  0.010  0.169506  0.815238\n",
            "5        (128,)      0.2  0.010  0.162589  0.808610\n",
            "3         (64,)      0.4  0.010  0.153898  0.827614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# 1) Load & preprocess as before\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_NAME']).dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
        "df.sort_values(['STORM_ID','TIMESTAMP'], inplace=True)\n",
        "\n",
        "features = [\n",
        "    'LAT','LON','VMAX_current','MSLP_current',\n",
        "    'DAY_OF_YEAR','STORM_AGE_hours',\n",
        "    'VMAX_prev_6hr','VMAX_prev_12hr','VMAX_prev_24hr',\n",
        "    'DELV_6hr','DELV_12hr','DELV_24hr',\n",
        "    'STORM_DIR','STORM_SPEED_kts',\n",
        "    'DIST_TO_LAND_km','SST_avg','MSLP_env_hPa',\n",
        "    'U10m_env_mps','V10m_env_mps','T2m_env_c','D2m_env_c'\n",
        "]\n",
        "cat_cols = ['STATUS']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ]), features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "X_all = preprocessor.fit_transform(df[features + cat_cols])\n",
        "y_all = df['TARGET_RI_24hr_30kt'].astype(int).values\n",
        "storms = df['STORM_ID'].values\n",
        "\n",
        "# 2) Stratified Storm split\n",
        "storm_labels = pd.Series(y_all, index=storms).groupby(level=0).max()\n",
        "pos_storms = storm_labels[storm_labels==1].index.to_numpy()\n",
        "neg_storms = storm_labels[storm_labels==0].index.to_numpy()\n",
        "rng = np.random.default_rng(42)\n",
        "val_pos = rng.choice(pos_storms, size=max(1,int(len(pos_storms)*0.2)), replace=False)\n",
        "val_neg = rng.choice(neg_storms, size=int(len(neg_storms)*0.2), replace=False)\n",
        "val_storms = np.concatenate([val_pos, val_neg])\n",
        "train_storms = np.setdiff1d(storm_labels.index.to_numpy(), val_storms)\n",
        "\n",
        "# 3) Sequence Dataset\n",
        "class StormSeqDataset(Dataset):\n",
        "    def __init__(self, X, y, storms, seq_len, storm_list):\n",
        "        self.data = []\n",
        "        for sid in storm_list:\n",
        "            mask = storms == sid\n",
        "            Xi, yi = X[mask], y[mask][-1]\n",
        "            if len(Xi) < seq_len:\n",
        "                pad = np.zeros((seq_len-len(Xi), Xi.shape[1]), dtype=Xi.dtype)\n",
        "                Xi = np.vstack([pad, Xi])\n",
        "            else:\n",
        "                Xi = Xi[-seq_len:]\n",
        "            self.data.append((Xi, yi))\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        Xseq, y = self.data[i]\n",
        "        return torch.tensor(Xseq, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "SEQ_LEN = 20\n",
        "train_ds = StormSeqDataset(X_all, y_all, storms, SEQ_LEN, train_storms)\n",
        "val_ds   = StormSeqDataset(X_all, y_all, storms, SEQ_LEN, val_storms)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "# 4) Temporal Convolutional Network (TCN)\n",
        "class TCNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation):\n",
        "        super().__init__()\n",
        "        padding = (kernel_size - 1) * dilation\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels,\n",
        "                              kernel_size,\n",
        "                              padding=padding,\n",
        "                              dilation=dilation)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.net = nn.Sequential(self.conv, self.relu)\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        return out[:, :, :x.size(2)]\n",
        "\n",
        "class TCNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_channels, kernel_size=3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        channels = [input_dim] + num_channels\n",
        "        for i in range(len(num_channels)):\n",
        "            layers.append(TCNBlock(channels[i], channels[i+1],\n",
        "                                   kernel_size, dilation=2**i))\n",
        "        self.tcn = nn.Sequential(*layers)\n",
        "        self.fc = nn.Linear(num_channels[-1], 2)\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, features) -> (batch, features, seq_len)\n",
        "        x = x.transpose(1,2)\n",
        "        out = self.tcn(x)\n",
        "        out = out[:, :, -1]\n",
        "        return self.fc(out)\n",
        "\n",
        "# 5) Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TCNClassifier(input_dim=X_all.shape[1],\n",
        "                      num_channels=[64,64,64]).to(device)\n",
        "\n",
        "# 6) Loss & optimizer\n",
        "neg = int((y_all==0).sum()); pos = int((y_all==1).sum())\n",
        "class_weight = torch.tensor([1.0, neg/pos], device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 7) Train & validate\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for Xb,yb in train_loader:\n",
        "        Xb,yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits,yb)\n",
        "        loss.backward(); optimizer.step()\n",
        "    # validation\n",
        "    model.eval()\n",
        "    preds, probs, labs = [],[],[]\n",
        "    with torch.no_grad():\n",
        "        for Xb,yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            p = torch.softmax(logits, dim=1)[:,1]\n",
        "            preds += (p>0.5).cpu().tolist()\n",
        "            probs += p.cpu().tolist()\n",
        "            labs  += yb.tolist()\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(classification_report(labs, preds))\n",
        "    print(\"Val ROC AUC:\", roc_auc_score(labs, probs))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnNkCUeDvXUL",
        "outputId": "83b934e3-7d9c-4af6-b5e4-ea457c57ef78"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n",
            "Epoch 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n",
            "Epoch 4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# — 1) Veri hazırlama (aynı pipeline ve stratified storm split) —\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_NAME']).dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
        "df.sort_values(['STORM_ID','TIMESTAMP'], inplace=True)\n",
        "\n",
        "features = [\n",
        "    'LAT','LON','VMAX_current','MSLP_current',\n",
        "    'DAY_OF_YEAR','STORM_AGE_hours',\n",
        "    'VMAX_prev_6hr','VMAX_prev_12hr','VMAX_prev_24hr',\n",
        "    'DELV_6hr','DELV_12hr','DELV_24hr',\n",
        "    'STORM_DIR','STORM_SPEED_kts',\n",
        "    'DIST_TO_LAND_km','SST_avg','MSLP_env_hPa',\n",
        "    'U10m_env_mps','V10m_env_mps','T2m_env_c','D2m_env_c'\n",
        "]\n",
        "cat_cols = ['STATUS']\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ]), features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "\n",
        "X_all = preprocessor.fit_transform(df[features + cat_cols])\n",
        "y_all = df['TARGET_RI_24hr_30kt'].astype(int).values\n",
        "storms = df['STORM_ID'].values\n",
        "\n",
        "# Stratified storm split\n",
        "storm_labels = pd.Series(y_all, index=storms).groupby(level=0).max()\n",
        "pos_storms = storm_labels[storm_labels==1].index.to_numpy()\n",
        "neg_storms = storm_labels[storm_labels==0].index.to_numpy()\n",
        "rng = np.random.default_rng(42)\n",
        "n_pos_val = max(1, int(len(pos_storms)*0.2))\n",
        "n_neg_val = int(len(neg_storms)*0.2)\n",
        "val_pos = rng.choice(pos_storms, size=n_pos_val, replace=False)\n",
        "val_neg = rng.choice(neg_storms, size=n_neg_val, replace=False)\n",
        "val_storms = np.concatenate([val_pos, val_neg])\n",
        "train_storms = np.setdiff1d(storm_labels.index.to_numpy(), val_storms)\n",
        "\n",
        "# StormSeqDataset (aynı)\n",
        "class StormSeqDataset(Dataset):\n",
        "    def __init__(self, X, y, storms, seq_len, storm_list):\n",
        "        self.data = []\n",
        "        for sid in storm_list:\n",
        "            mask = storms == sid\n",
        "            Xi, yi = X[mask], y[mask][-1]\n",
        "            if len(Xi) < seq_len:\n",
        "                pad = np.zeros((seq_len-len(Xi), Xi.shape[1]), dtype=Xi.dtype)\n",
        "                Xi = np.vstack([pad, Xi])\n",
        "            else:\n",
        "                Xi = Xi[-seq_len:]\n",
        "            self.data.append((Xi, yi))\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, i):\n",
        "        Xseq, y = self.data[i]\n",
        "        return torch.tensor(Xseq, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "SEQ_LEN = 20\n",
        "train_ds = StormSeqDataset(X_all, y_all, storms, SEQ_LEN, train_storms)\n",
        "val_ds   = StormSeqDataset(X_all, y_all, storms, SEQ_LEN, val_storms)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "# — 2) Transformer Encoder Model —\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, feature_dim, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout, activation='relu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, feature_dim)\n",
        "        x = self.input_proj(x)          # → (batch, seq_len, d_model)\n",
        "        x = x.permute(1,0,2)            # → (seq_len, batch, d_model) for transformer\n",
        "        h = self.transformer(x)         # → (seq_len, batch, d_model)\n",
        "        out = h[-1]                     # take last time step: (batch, d_model)\n",
        "        return self.classifier(out)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TimeSeriesTransformer(feature_dim=X_all.shape[1]).to(device)\n",
        "\n",
        "# — 3) Loss, optimizer and class weights —\n",
        "neg = int((y_all==0).sum()); pos = int((y_all==1).sum())\n",
        "class_weight = torch.tensor([1.0, neg/pos], device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# — 4) Training & Validation Loop —\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    preds, probs, labs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            p = torch.softmax(logits, dim=1)[:,1]\n",
        "            preds += (p>0.5).cpu().tolist()\n",
        "            probs += p.cpu().tolist()\n",
        "            labs  += yb.tolist()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(classification_report(labs, preds))\n",
        "    print(\"Val ROC AUC:\", roc_auc_score(labs, probs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlICez6fwejk",
        "outputId": "7653b985-0cb0-44af-92a5-08f8dd3e8429"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       388\n",
            "\n",
            "    accuracy                           1.00       388\n",
            "   macro avg       1.00      1.00      1.00       388\n",
            "weighted avg       1.00      1.00      1.00       388\n",
            "\n",
            "Val ROC AUC: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# 1) Load & clean data\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_NAME']).dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
        "df.sort_values(['STORM_ID','TIMESTAMP'], inplace=True)\n",
        "\n",
        "# 2) Feature lists\n",
        "features = [\n",
        "    'LAT','LON','VMAX_current','MSLP_current',\n",
        "    'DAY_OF_YEAR','STORM_AGE_hours',\n",
        "    'VMAX_prev_6hr','VMAX_prev_12hr','VMAX_prev_24hr',\n",
        "    'DELV_6hr','DELV_12hr','DELV_24hr',\n",
        "    'STORM_DIR','STORM_SPEED_kts',\n",
        "    'DIST_TO_LAND_km','SST_avg','MSLP_env_hPa',\n",
        "    'U10m_env_mps','V10m_env_mps','T2m_env_c','D2m_env_c'\n",
        "]\n",
        "cat_cols = ['STATUS']\n",
        "\n",
        "# 3) Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ]), features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "\n",
        "# 4) Transform full dataset\n",
        "X_all = preprocessor.fit_transform(df[features + cat_cols])\n",
        "y_all = df['TARGET_RI_24hr_30kt'].astype(int).values\n",
        "storms = df['STORM_ID'].values\n",
        "\n",
        "# 5) Stratified storm-level split\n",
        "storm_labels = pd.Series(y_all, index=storms).groupby(level=0).max()\n",
        "pos_storms = storm_labels[storm_labels==1].index.to_numpy()\n",
        "neg_storms = storm_labels[storm_labels==0].index.to_numpy()\n",
        "rng = np.random.default_rng(42)\n",
        "n_pos_val = max(1, int(len(pos_storms)*0.2))\n",
        "n_neg_val = int(len(neg_storms)*0.2)\n",
        "val_pos = rng.choice(pos_storms, size=n_pos_val, replace=False)\n",
        "val_neg = rng.choice(neg_storms, size=n_neg_val, replace=False)\n",
        "val_storms = np.concatenate([val_pos, val_neg])\n",
        "train_storms = np.setdiff1d(storm_labels.index.to_numpy(), val_storms)\n",
        "\n",
        "# 6) Storm label dict\n",
        "storm_label_dict = storm_labels.to_dict()\n",
        "\n",
        "# 7) Dataset class using storm_label_dict\n",
        "class StormSeqDataset(Dataset):\n",
        "    def __init__(self, X, storms, seq_len, storm_list, label_dict):\n",
        "        self.data = []\n",
        "        for sid in storm_list:\n",
        "            mask = storms == sid\n",
        "            Xi = X[mask]\n",
        "            yi = label_dict[sid]\n",
        "            if len(Xi) < seq_len:\n",
        "                pad = np.zeros((seq_len - len(Xi), Xi.shape[1]), dtype=Xi.dtype)\n",
        "                Xi = np.vstack([pad, Xi])\n",
        "            else:\n",
        "                Xi = Xi[-seq_len:]\n",
        "            self.data.append((Xi, yi))\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        Xseq, y = self.data[idx]\n",
        "        return torch.tensor(Xseq, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 8) DataLoaders\n",
        "SEQ_LEN = 20\n",
        "train_ds = StormSeqDataset(X_all, storms, SEQ_LEN, train_storms, storm_label_dict)\n",
        "val_ds   = StormSeqDataset(X_all, storms, SEQ_LEN, val_storms, storm_label_dict)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "# 9) Transformer Encoder model\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, feature_dim, d_model=64, nhead=4,\n",
        "                 num_layers=2, dim_feedforward=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        h = self.transformer(x)\n",
        "        return self.classifier(h[:, -1, :])\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TimeSeriesTransformer(feature_dim=X_all.shape[1]).to(device)\n",
        "\n",
        "# 10) Loss & optimizer\n",
        "neg = int((y_all==0).sum()); pos = int((y_all==1).sum())\n",
        "class_weight = torch.tensor([1.0, neg/pos], device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 11) Training & Validation loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    preds, probs, labs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            p = torch.softmax(logits, dim=1)[:,1]\n",
        "            preds += (p>0.5).cpu().tolist()\n",
        "            probs += p.cpu().tolist()\n",
        "            labs  += yb.tolist()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(classification_report(labs, preds))\n",
        "    print(\"Val ROC AUC:\", roc_auc_score(labs, probs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kz5FgS4wf2C",
        "outputId": "782ed0a0-cb05-415e-95d1-8ce561b27fa3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.13      0.23       284\n",
            "           1       0.30      1.00      0.46       104\n",
            "\n",
            "    accuracy                           0.36       388\n",
            "   macro avg       0.65      0.56      0.34       388\n",
            "weighted avg       0.81      0.36      0.29       388\n",
            "\n",
            "Val ROC AUC: 0.8394840195016251\n",
            "Epoch 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.52      0.68       284\n",
            "           1       0.42      0.96      0.59       104\n",
            "\n",
            "    accuracy                           0.64       388\n",
            "   macro avg       0.70      0.74      0.63       388\n",
            "weighted avg       0.83      0.64      0.65       388\n",
            "\n",
            "Val ROC AUC: 0.8720205850487541\n",
            "Epoch 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.52      0.68       284\n",
            "           1       0.43      0.97      0.59       104\n",
            "\n",
            "    accuracy                           0.64       388\n",
            "   macro avg       0.70      0.75      0.64       388\n",
            "weighted avg       0.83      0.64      0.66       388\n",
            "\n",
            "Val ROC AUC: 0.886951516793066\n",
            "Epoch 4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.53      0.69       284\n",
            "           1       0.44      1.00      0.61       104\n",
            "\n",
            "    accuracy                           0.65       388\n",
            "   macro avg       0.72      0.76      0.65       388\n",
            "weighted avg       0.85      0.65      0.67       388\n",
            "\n",
            "Val ROC AUC: 0.9175920910075841\n",
            "Epoch 5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.60      0.75       284\n",
            "           1       0.48      0.99      0.64       104\n",
            "\n",
            "    accuracy                           0.71       388\n",
            "   macro avg       0.74      0.80      0.70       388\n",
            "weighted avg       0.86      0.71      0.72       388\n",
            "\n",
            "Val ROC AUC: 0.9480633802816902\n",
            "Epoch 6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.61      0.75       284\n",
            "           1       0.48      0.99      0.65       104\n",
            "\n",
            "    accuracy                           0.71       388\n",
            "   macro avg       0.74      0.80      0.70       388\n",
            "weighted avg       0.86      0.71      0.72       388\n",
            "\n",
            "Val ROC AUC: 0.9386511375947996\n",
            "Epoch 7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.77      0.86       284\n",
            "           1       0.60      0.97      0.75       104\n",
            "\n",
            "    accuracy                           0.82       388\n",
            "   macro avg       0.80      0.87      0.80       388\n",
            "weighted avg       0.88      0.82      0.83       388\n",
            "\n",
            "Val ROC AUC: 0.955816630552546\n",
            "Epoch 8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.66      0.79       284\n",
            "           1       0.52      0.99      0.68       104\n",
            "\n",
            "    accuracy                           0.75       388\n",
            "   macro avg       0.76      0.83      0.74       388\n",
            "weighted avg       0.87      0.75      0.76       388\n",
            "\n",
            "Val ROC AUC: 0.947453954496208\n",
            "Epoch 9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.82      0.89       284\n",
            "           1       0.66      0.96      0.78       104\n",
            "\n",
            "    accuracy                           0.86       388\n",
            "   macro avg       0.82      0.89      0.84       388\n",
            "weighted avg       0.90      0.86      0.86       388\n",
            "\n",
            "Val ROC AUC: 0.971830985915493\n",
            "Epoch 10\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.69      0.81       284\n",
            "           1       0.53      0.98      0.69       104\n",
            "\n",
            "    accuracy                           0.77       388\n",
            "   macro avg       0.76      0.83      0.75       388\n",
            "weighted avg       0.87      0.77      0.78       388\n",
            "\n",
            "Val ROC AUC: 0.9397345612134345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# 1) Load & clean data\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_NAME']).dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
        "df.sort_values(['STORM_ID','TIMESTAMP'], inplace=True)\n",
        "\n",
        "# 2) Feature and target arrays\n",
        "features = [\n",
        "    'LAT','LON','VMAX_current','MSLP_current',\n",
        "    'DAY_OF_YEAR','STORM_AGE_hours',\n",
        "    'VMAX_prev_6hr','VMAX_prev_12hr','VMAX_prev_24hr',\n",
        "    'DELV_6hr','DELV_12hr','DELV_24hr',\n",
        "    'STORM_DIR','STORM_SPEED_kts',\n",
        "    'DIST_TO_LAND_km','SST_avg','MSLP_env_hPa',\n",
        "    'U10m_env_mps','V10m_env_mps','T2m_env_c','D2m_env_c'\n",
        "]\n",
        "cat_cols = ['STATUS']\n",
        "\n",
        "# 3) Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ]), features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "\n",
        "X_all = preprocessor.fit_transform(df[features + cat_cols])\n",
        "y_all = df['TARGET_RI_24hr_30kt'].astype(int).values\n",
        "storms = df['STORM_ID'].values\n",
        "\n",
        "# 4) Stratified storm-level split\n",
        "storm_labels = pd.Series(y_all, index=storms).groupby(level=0).max()\n",
        "pos_storms = storm_labels[storm_labels==1].index.to_numpy()\n",
        "neg_storms = storm_labels[storm_labels==0].index.to_numpy()\n",
        "rng = np.random.default_rng(42)\n",
        "n_pos_val = max(1, int(len(pos_storms)*0.2))\n",
        "n_neg_val = int(len(neg_storms)*0.2)\n",
        "val_pos = rng.choice(pos_storms, size=n_pos_val, replace=False)\n",
        "val_neg = rng.choice(neg_storms, size=n_neg_val, replace=False)\n",
        "val_storms = np.concatenate([val_pos, val_neg])\n",
        "train_storms = np.setdiff1d(storm_labels.index.to_numpy(), val_storms)\n",
        "\n",
        "# 5) Storm label dict (max across sequence)\n",
        "storm_label_dict = storm_labels.to_dict()\n",
        "\n",
        "# 6) Dataset class using storm_label_dict\n",
        "class StormSeqDataset(Dataset):\n",
        "    def __init__(self, X, storms, seq_len, storm_list, label_dict):\n",
        "        self.data = []\n",
        "        for sid in storm_list:\n",
        "            mask = storms == sid\n",
        "            Xi = X[mask]\n",
        "            yi = label_dict[sid]\n",
        "            if len(Xi) < seq_len:\n",
        "                pad = np.zeros((seq_len - len(Xi), Xi.shape[1]), dtype=Xi.dtype)\n",
        "                Xi = np.vstack([pad, Xi])\n",
        "            else:\n",
        "                Xi = Xi[-seq_len:]\n",
        "            self.data.append((Xi, yi))\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        Xseq, y = self.data[idx]\n",
        "        return torch.tensor(Xseq, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 7) DataLoaders\n",
        "SEQ_LEN = 20\n",
        "train_ds = StormSeqDataset(X_all, storms, SEQ_LEN, train_storms, storm_label_dict)\n",
        "val_ds   = StormSeqDataset(X_all, storms, SEQ_LEN, val_storms, storm_label_dict)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32)\n",
        "\n",
        "# 8) Force CPU device and correct class_weight dtype\n",
        "device = torch.device('cpu')\n",
        "neg = int((y_all==0).sum())\n",
        "pos = int((y_all==1).sum())\n",
        "class_weight = torch.tensor([1.0, neg/pos], dtype=torch.float32, device=device)\n",
        "\n",
        "# 9) LSTM Model Definition\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc   = nn.Linear(hidden_dim, 2)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "model_lstm = LSTMClassifier(input_dim=X_all.shape[1], hidden_dim=64).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "optimizer = optim.Adam(model_lstm.parameters(), lr=1e-3)\n",
        "\n",
        "# 10) Train & Validate LSTM\n",
        "print(\"Training LSTM...\")\n",
        "for epoch in range(5):\n",
        "    model_lstm.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model_lstm(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Validation\n",
        "    model_lstm.eval()\n",
        "    preds, probs, labs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            logits = model_lstm(Xb.to(device))\n",
        "            p = torch.softmax(logits, dim=1)[:,1]\n",
        "            preds += (p>0.5).cpu().tolist()\n",
        "            probs += p.cpu().tolist()\n",
        "            labs  += yb.tolist()\n",
        "    print(f\"LSTM Epoch {epoch+1} AUC: {roc_auc_score(labs, probs):.4f}\")\n",
        "print(\"LSTM Classification Report:\\n\", classification_report(labs, preds))\n",
        "\n",
        "# 11) TCN Model Definition\n",
        "class TCNBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k, d):\n",
        "        super().__init__()\n",
        "        pad = (k-1)*d\n",
        "        self.conv = nn.Conv1d(in_ch, out_ch, k, padding=pad, dilation=d)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        y = self.relu(self.conv(x))\n",
        "        return y[:, :, :x.size(2)]\n",
        "\n",
        "class TCNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, channels=[64,64,64], kernel=3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        chs = [input_dim] + channels\n",
        "        for i in range(len(channels)):\n",
        "            layers.append(TCNBlock(chs[i], chs[i+1], kernel, 2**i))\n",
        "        self.tcn = nn.Sequential(*layers)\n",
        "        self.fc  = nn.Linear(channels[-1], 2)\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1,2)\n",
        "        out = self.tcn(x)\n",
        "        return self.fc(out[:, :, -1])\n",
        "\n",
        "model_tcn = TCNClassifier(input_dim=X_all.shape[1]).to(device)\n",
        "optimizer_tcn = optim.Adam(model_tcn.parameters(), lr=1e-3)\n",
        "\n",
        "# 12) Train & Validate TCN\n",
        "print(\"Training TCN...\")\n",
        "for epoch in range(5):\n",
        "    model_tcn.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        optimizer_tcn.zero_grad()\n",
        "        logits = model_tcn(Xb.to(device))\n",
        "        loss = criterion(logits, yb.to(device))\n",
        "        loss.backward()\n",
        "        optimizer_tcn.step()\n",
        "    # Validation\n",
        "    model_tcn.eval()\n",
        "    preds, probs, labs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            logits = model_tcn(Xb.to(device))\n",
        "            p = torch.softmax(logits, dim=1)[:,1]\n",
        "            preds += (p>0.5).cpu().tolist()\n",
        "            probs += p.cpu().tolist()\n",
        "            labs  += yb.tolist()\n",
        "    print(f\"TCN Epoch {epoch+1} AUC: {roc_auc_score(labs, probs):.4f}\")\n",
        "print(\"TCN Classification Report:\\n\", classification_report(labs, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNMrBH8Yw2DR",
        "outputId": "09f41d24-2672-4a77-e253-46ef2a34782c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM...\n",
            "LSTM Epoch 1 AUC: 0.8054\n",
            "LSTM Epoch 2 AUC: 0.8377\n",
            "LSTM Epoch 3 AUC: 0.8610\n",
            "LSTM Epoch 4 AUC: 0.8906\n",
            "LSTM Epoch 5 AUC: 0.8969\n",
            "LSTM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.31      0.47       284\n",
            "           1       0.34      0.99      0.51       104\n",
            "\n",
            "    accuracy                           0.49       388\n",
            "   macro avg       0.67      0.65      0.49       388\n",
            "weighted avg       0.82      0.49      0.48       388\n",
            "\n",
            "Training TCN...\n",
            "TCN Epoch 1 AUC: 0.8772\n",
            "TCN Epoch 2 AUC: 0.9128\n",
            "TCN Epoch 3 AUC: 0.9260\n",
            "TCN Epoch 4 AUC: 0.9366\n",
            "TCN Epoch 5 AUC: 0.9481\n",
            "TCN Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.60      0.75       284\n",
            "           1       0.48      1.00      0.65       104\n",
            "\n",
            "    accuracy                           0.71       388\n",
            "   macro avg       0.74      0.80      0.70       388\n",
            "weighted avg       0.86      0.71      0.72       388\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# 1) Load & clean data\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_NAME']).dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
        "df.sort_values(['STORM_ID','TIMESTAMP'], inplace=True)\n",
        "\n",
        "# 2) Features and categorical\n",
        "features = [\n",
        "    'LAT','LON','VMAX_current','MSLP_current',\n",
        "    'DAY_OF_YEAR','STORM_AGE_hours',\n",
        "    'VMAX_prev_6hr','VMAX_prev_12hr','VMAX_prev_24hr',\n",
        "    'DELV_6hr','DELV_12hr','DELV_24hr',\n",
        "    'STORM_DIR','STORM_SPEED_kts',\n",
        "    'DIST_TO_LAND_km','SST_avg','MSLP_env_hPa',\n",
        "    'U10m_env_mps','V10m_env_mps','T2m_env_c','D2m_env_c'\n",
        "]\n",
        "cat_cols = ['STATUS']\n",
        "\n",
        "# 3) Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([('imp', SimpleImputer(strategy='median')), ('scale', StandardScaler())]), features),\n",
        "    ('cat', Pipeline([('imp', SimpleImputer(strategy='most_frequent')),\n",
        "                      ('enc', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), cat_cols)\n",
        "])\n",
        "\n",
        "X_all = preprocessor.fit_transform(df[features + cat_cols])\n",
        "y_all = df['TARGET_RI_24hr_30kt'].astype(int).values\n",
        "storms = df['STORM_ID'].values\n",
        "\n",
        "# 4) Triple split at storm-level (20% test, 20% val, rest train), stratified by max label\n",
        "storm_labels = pd.Series(y_all, index=storms).groupby(level=0).max()\n",
        "pos = storm_labels[storm_labels==1].index.to_numpy()\n",
        "neg = storm_labels[storm_labels==0].index.to_numpy()\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# test storms\n",
        "n_pos_test = max(1,int(len(pos)*0.2))\n",
        "n_neg_test = int(len(neg)*0.2)\n",
        "test_pos = rng.choice(pos, size=n_pos_test, replace=False)\n",
        "test_neg = rng.choice(neg, size=n_neg_test, replace=False)\n",
        "test_storms = np.concatenate([test_pos, test_neg])\n",
        "\n",
        "# remaining for train+val\n",
        "remaining = np.setdiff1d(storm_labels.index.to_numpy(), test_storms)\n",
        "\n",
        "# val storms from remaining\n",
        "pos_rem = np.intersect1d(remaining, pos)\n",
        "neg_rem = np.intersect1d(remaining, neg)\n",
        "n_pos_val = max(1,int(len(pos)*0.2))\n",
        "n_neg_val = int(len(neg)*0.2)\n",
        "val_pos = rng.choice(pos_rem, size=n_pos_val, replace=False)\n",
        "val_neg = rng.choice(neg_rem, size=n_neg_val, replace=False)\n",
        "val_storms = np.concatenate([val_pos, val_neg])\n",
        "\n",
        "# train storms = rest\n",
        "train_storms = np.setdiff1d(remaining, val_storms)\n",
        "\n",
        "# 5) Label dict\n",
        "storm_label_dict = storm_labels.to_dict()\n",
        "\n",
        "# 6) Sequence dataset\n",
        "class StormSeqDataset(Dataset):\n",
        "    def __init__(self, X, storms, seq_len, storm_list, label_dict):\n",
        "        self.data = []\n",
        "        for sid in storm_list:\n",
        "            mask = storms==sid\n",
        "            Xi = X[mask]\n",
        "            yi = label_dict[sid]\n",
        "            if len(Xi)<seq_len:\n",
        "                pad = np.zeros((seq_len-len(Xi), Xi.shape[1]), dtype=Xi.dtype)\n",
        "                Xi = np.vstack([pad, Xi])\n",
        "            else:\n",
        "                Xi = Xi[-seq_len:]\n",
        "            self.data.append((Xi, yi))\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        Xseq, y = self.data[idx]\n",
        "        return torch.tensor(Xseq, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "SEQ_LEN = 20\n",
        "train_ds = StormSeqDataset(X_all, storms, SEQ_LEN, train_storms, storm_label_dict)\n",
        "val_ds   = StormSeqDataset(X_all, storms, SEQ_LEN, val_storms,   storm_label_dict)\n",
        "test_ds  = StormSeqDataset(X_all, storms, SEQ_LEN, test_storms,  storm_label_dict)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32)\n",
        "\n",
        "# 7) Transformer model\n",
        "class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, feature_dim, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
        "        enc_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation='relu', batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        h = self.transformer(x)\n",
        "        return self.classifier(h[:, -1, :])\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TimeSeriesTransformer(feature_dim=X_all.shape[1]).to(device)\n",
        "\n",
        "# 8) Loss & optimizer\n",
        "neg_count = int((y_all==0).sum()); pos_count = int((y_all==1).sum())\n",
        "class_weight = torch.tensor([1.0, neg_count/pos_count], dtype=torch.float32, device=device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 9) Train on train, validate for best\n",
        "best_auc = 0; best_state = None\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(Xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # validation\n",
        "    model.eval()\n",
        "    preds, probs, labs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            logits = model(Xb)\n",
        "            p = torch.softmax(logits, dim=1)[:,1]\n",
        "            preds += (p>0.5).cpu().tolist()\n",
        "            probs += p.cpu().tolist()\n",
        "            labs  += yb.tolist()\n",
        "    auc = roc_auc_score(labs, probs)\n",
        "    if auc > best_auc:\n",
        "        best_auc = auc\n",
        "        best_state = model.state_dict()\n",
        "print(f\"Best validation ROC AUC: {best_auc:.4f}\")\n",
        "\n",
        "# load best model\n",
        "model.load_state_dict(best_state)\n",
        "\n",
        "# 10) Test set evaluation\n",
        "model.eval()\n",
        "preds, probs, labs = [], [], []\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        logits = model(Xb.to(device))\n",
        "        p = torch.softmax(logits, dim=1)[:,1]\n",
        "        preds += (p>0.5).cpu().tolist()\n",
        "        probs += p.cpu().tolist()\n",
        "        labs  += yb.tolist()\n",
        "\n",
        "print(\"Test ROC AUC:\", roc_auc_score(labs, probs))\n",
        "print(\"Test Classification Report:\\n\", classification_report(labs, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvLy4-6vxQM6",
        "outputId": "1fa2d00a-f478-483f-ee9b-83117730b8ee"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best validation ROC AUC: 0.9476\n",
            "Test ROC AUC: 0.9624526002166848\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.77      0.87       284\n",
            "           1       0.61      0.97      0.75       104\n",
            "\n",
            "    accuracy                           0.83       388\n",
            "   macro avg       0.80      0.87      0.81       388\n",
            "weighted avg       0.89      0.83      0.84       388\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
        "\n",
        "# Assuming you have the true labels and predicted probabilities from the test set:\n",
        "# y_true: list or array of true labels (0/1)\n",
        "# y_scores: list or array of predicted probabilities for class 1\n",
        "\n",
        "\n",
        "y_true, y_scores = [], []\n",
        "for Xb, yb in test_loader:\n",
        "     logits = model(Xb.to(device))\n",
        "     probs = torch.softmax(logits, dim=1)[:,1].cpu().tolist()\n",
        "     y_scores.extend(probs)\n",
        "     y_true.extend(yb.tolist())\n",
        "\n",
        "# Threshold sweep\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "# Assemble results\n",
        "import pandas as pd\n",
        "df_thresh = pd.DataFrame({\n",
        "    'threshold': np.append(thresholds, 1.0),  # last threshold for recall=0\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1': f1_scores\n",
        "})\n",
        "\n",
        "# Find best threshold by F1\n",
        "best_idx = df_thresh['f1'].idxmax()\n",
        "best_row = df_thresh.loc[best_idx]\n",
        "\n",
        "# Display\n",
        "\n",
        "print(f\"Best threshold: {best_row['threshold']:.3f} with F1 = {best_row['f1']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wrNKncFx7bz",
        "outputId": "1f4c6557-d101-4888-e0b2-08c682497c05"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.976 with F1 = 0.829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# 1) Collect true labels and predicted probabilities\n",
        "y_true, y_scores = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        logits = model(Xb)\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1]\n",
        "        y_scores.extend(probs.cpu().tolist())\n",
        "        y_true.extend(yb.tolist())\n",
        "\n",
        "# 2) Apply optimized threshold\n",
        "best_threshold = 0.976\n",
        "y_pred_thr = [1 if score > best_threshold else 0 for score in y_scores]\n",
        "\n",
        "# 3) Compute report and confusion matrix\n",
        "report_thr = classification_report(y_true, y_pred_thr, output_dict=True)\n",
        "df_report_thr = pd.DataFrame(report_thr).transpose()\n",
        "\n",
        "conf_matrix_thr = confusion_matrix(y_true, y_pred_thr)\n",
        "auc_score = roc_auc_score(y_true, y_scores)\n",
        "\n",
        "# 4) Display results\n",
        "\n",
        "print(df_report_thr)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix_thr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLYz0G4XymMZ",
        "outputId": "0d04b7ad-4660-4d40-e0db-97cd973ede9b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score     support\n",
            "0              0.936396  0.933099  0.934744  284.000000\n",
            "1              0.819048  0.826923  0.822967  104.000000\n",
            "accuracy       0.904639  0.904639  0.904639    0.904639\n",
            "macro avg      0.877722  0.880011  0.878855  388.000000\n",
            "weighted avg   0.904942  0.904639  0.904783  388.000000\n",
            "Confusion Matrix:\n",
            " [[265  19]\n",
            " [ 18  86]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# 1) Veri yükleme ve temizleme\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_NAME']).dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "\n",
        "# 2) Özellik / target ayrımı (STORM_ID ve TIMESTAMP’i de atıyoruz)\n",
        "X = df.drop(columns=[\n",
        "    'STORM_ID','TIMESTAMP',\n",
        "    'TARGET_DELV_24hr','TARGET_RI_24hr_30kt'\n",
        "])\n",
        "y = df['TARGET_RI_24hr_30kt'].astype(int)\n",
        "\n",
        "# 3) Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 4) Preprocessing pipeline\n",
        "cat_cols = ['STATUS']\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ]), num_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "\n",
        "# 5) Base learners (GPU hızlandırmalı)\n",
        "scale = (y_train==0).sum() / (y_train==1).sum()\n",
        "estimators = [\n",
        "    ('xgb', XGBClassifier(\n",
        "        tree_method='gpu_hist', predictor='gpu_predictor',\n",
        "        learning_rate=0.1, max_depth=9, n_estimators=300,\n",
        "        scale_pos_weight=scale,\n",
        "        random_state=42, use_label_encoder=False, eval_metric='logloss'\n",
        "    )),\n",
        "    ('lgb', LGBMClassifier(\n",
        "        device='gpu', gpu_device_id=0,\n",
        "        learning_rate=0.1, n_estimators=500, num_leaves=31,\n",
        "        scale_pos_weight=scale, random_state=42\n",
        "    )),\n",
        "    ('cb', CatBoostClassifier(\n",
        "        task_type='GPU', devices='0',\n",
        "        learning_rate=0.1, depth=8, iterations=300,\n",
        "        scale_pos_weight=scale, random_state=42, verbose=False\n",
        "    ))\n",
        "]\n",
        "\n",
        "# 6) Stacking meta-model\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(\n",
        "        class_weight='balanced', max_iter=1000\n",
        "    ),\n",
        "    cv=5,\n",
        "    n_jobs=1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('stack', stack)\n",
        "])\n",
        "\n",
        "# 7) Eğitim ve değerlendirme\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "y_proba = pipe.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"Stacking ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Stacking Classification Report:\\n\",\n",
        "      classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgifuD6HyqXd",
        "outputId": "c7219f24-1b8f-4dbc-b796-2d747f5ad6be"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1385, number of negative: 36457\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3555\n",
            "[LightGBM] [Info] Number of data points in the train set: 37842, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.87 MB) transferred to GPU in 0.003686 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270433\n",
            "[LightGBM] [Info] Start training from score -3.270433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:17:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:17:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:17:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:17:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:17:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:17:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001685 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3548\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001537 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3547\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001578 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3552\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.001585 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3549\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 30\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 22 dense feature groups (0.69 MB) transferred to GPU in 0.002747 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:18:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking ROC AUC: 0.9097837839551777\n",
            "Stacking Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.88      0.93      9115\n",
            "           1       0.20      0.77      0.31       346\n",
            "\n",
            "    accuracy                           0.88      9461\n",
            "   macro avg       0.59      0.83      0.62      9461\n",
            "weighted avg       0.96      0.88      0.91      9461\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# 1) Load & sort data\n",
        "df = pd.read_csv('hurricanes_s11_env_dewpoint2m.csv')\n",
        "df = df.drop(columns=['RECORD_ID','STORM_NAME']).dropna(subset=['TARGET_RI_24hr_30kt'])\n",
        "df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])\n",
        "df.sort_values(['STORM_ID','TIMESTAMP'], inplace=True)\n",
        "\n",
        "# 2) Preprocessor for Transformer features\n",
        "features = [\n",
        "    'LAT','LON','VMAX_current','MSLP_current',\n",
        "    'DAY_OF_YEAR','STORM_AGE_hours',\n",
        "    'VMAX_prev_6hr','VMAX_prev_12hr','VMAX_prev_24hr',\n",
        "    'DELV_6hr','DELV_12hr','DELV_24hr',\n",
        "    'STORM_DIR','STORM_SPEED_kts',\n",
        "    'DIST_TO_LAND_km','SST_avg','MSLP_env_hPa',\n",
        "    'U10m_env_mps','V10m_env_mps','T2m_env_c','D2m_env_c'\n",
        "]\n",
        "cat_cols = ['STATUS']\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ]), features),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "X_all = preprocessor.fit_transform(df[features + cat_cols])\n",
        "storms = df['STORM_ID'].values\n",
        "\n",
        "# 3) Load pretrained Transformer model and move to device\n",
        "# (Assuming `model` is defined and its state loaded)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 4) Compute storm-level Transformer probability\n",
        "SEQ_LEN = 20\n",
        "storm_proba = {}\n",
        "for sid in np.unique(storms):\n",
        "    Xi = X_all[storms == sid]\n",
        "    if Xi.shape[0] < SEQ_LEN:\n",
        "        pad = np.zeros((SEQ_LEN - Xi.shape[0], Xi.shape[1]), dtype=Xi.dtype)\n",
        "        Xi = np.vstack([pad, Xi])\n",
        "    else:\n",
        "        Xi = Xi[-SEQ_LEN:]\n",
        "    Xi_t = torch.tensor(Xi, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(Xi_t)\n",
        "        p = torch.softmax(logits, dim=1)[0,1].item()\n",
        "    storm_proba[sid] = p\n",
        "\n",
        "df['trans_proba'] = df['STORM_ID'].map(storm_proba)\n",
        "\n",
        "# 5) Prepare stacking dataset\n",
        "X = df.drop(columns=['STORM_ID','TIMESTAMP','TARGET_DELV_24hr','TARGET_RI_24hr_30kt'])\n",
        "y = df['TARGET_RI_24hr_30kt'].astype(int)\n",
        "\n",
        "# 6) Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 7) Preprocessor for stacking\n",
        "cat_cols = ['STATUS']\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "preproc_stack = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ]), num_cols),\n",
        "    ('cat', Pipeline([\n",
        "        ('imp', SimpleImputer(strategy='most_frequent')),\n",
        "        ('enc', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), cat_cols)\n",
        "])\n",
        "\n",
        "# 8) Define base learners\n",
        "scale = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "estimators = [\n",
        "    ('xgb', XGBClassifier(\n",
        "        tree_method='gpu_hist', predictor='gpu_predictor',\n",
        "        learning_rate=0.1, max_depth=9, n_estimators=300,\n",
        "        scale_pos_weight=scale, random_state=42,\n",
        "        use_label_encoder=False, eval_metric='logloss'\n",
        "    )),\n",
        "    ('lgb', LGBMClassifier(\n",
        "        device='gpu', gpu_device_id=0,\n",
        "        learning_rate=0.1, n_estimators=500, num_leaves=31,\n",
        "        scale_pos_weight=scale, random_state=42\n",
        "    )),\n",
        "    ('cb', CatBoostClassifier(\n",
        "        task_type='GPU', devices='0',\n",
        "        learning_rate=0.1, depth=8, iterations=300,\n",
        "        scale_pos_weight=scale, random_state=42, verbose=False\n",
        "    ))\n",
        "]\n",
        "\n",
        "# 9) Create stacking ensemble\n",
        "stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(class_weight='balanced', max_iter=1000),\n",
        "    cv=5, n_jobs=1, passthrough=False\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('prep', preproc_stack),\n",
        "    ('stack', stack)\n",
        "])\n",
        "\n",
        "# 10) Train & evaluate\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "y_proba = pipe.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"Stack+Trans ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Stack+Trans Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRWrEp4hzK0D",
        "outputId": "0ecf7549-0106-4c43-fdfd-cdcc657a8116"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1385, number of negative: 36457\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3810\n",
            "[LightGBM] [Info] Number of data points in the train set: 37842, number of used features: 31\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 23 dense feature groups (0.87 MB) transferred to GPU in 0.001950 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270433\n",
            "[LightGBM] [Info] Start training from score -3.270433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:21:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:21:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:21:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:21:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [18:21:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"predictor\", \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:21:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3803\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 31\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 23 dense feature groups (0.69 MB) transferred to GPU in 0.001831 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29165\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3803\n",
            "[LightGBM] [Info] Number of data points in the train set: 30273, number of used features: 31\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 23 dense feature groups (0.69 MB) transferred to GPU in 0.003094 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036600 -> initscore=-3.270413\n",
            "[LightGBM] [Info] Start training from score -3.270413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3802\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 31\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 23 dense feature groups (0.69 MB) transferred to GPU in 0.002858 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3807\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 31\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 23 dense feature groups (0.69 MB) transferred to GPU in 0.001697 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1108, number of negative: 29166\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 3804\n",
            "[LightGBM] [Info] Number of data points in the train set: 30274, number of used features: 31\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 23 dense feature groups (0.69 MB) transferred to GPU in 0.001575 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036599 -> initscore=-3.270447\n",
            "[LightGBM] [Info] Start training from score -3.270447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:2676: UserWarning: [18:22:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  if len(data.shape) != 1 and self.num_features() != data.shape[1]:\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stack+Trans ROC AUC: 0.9423465100720086\n",
            "Stack+Trans Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95      9115\n",
            "           1       0.27      0.83      0.41       346\n",
            "\n",
            "    accuracy                           0.91      9461\n",
            "   macro avg       0.63      0.87      0.68      9461\n",
            "weighted avg       0.97      0.91      0.93      9461\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Na3T9tdAz7jb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}